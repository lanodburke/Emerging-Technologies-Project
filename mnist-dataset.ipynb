{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset Notebook\n",
    "The MNIST dataset is a dataset that is built up of hand written digits derived from the NIST dataset. It is used for people to derive machine learning models for pattern recognition from a real world set of data. \n",
    "\n",
    "The MNIST data set is widely used for training image recognition classifiers. \n",
    "\n",
    "The dataset consists of:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "\n",
    "The dataset can be found on this website [here](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        display: inline-block\n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset File format\n",
    "> All the integers in the files are stored in the MSB first (high endian) format used by most non-Intel processors. Users of Intel processors and other low-endian machines must flip the bytes of the header.\n",
    "\n",
    "The MNIST dataset is stored with the IDX file format extension. The IDX file format is a simple format for vectors and multidimensional matrices of various numerical types. \n",
    "\n",
    "The MNIST dataset consists of four files outlined below:\n",
    "\n",
    "| File          | Description       | \n",
    "| :------------ | ----------------- | \n",
    "| [train-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz) | training set images (9912422 bytes) | \n",
    "| [train-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz) | training set labels (28881 bytes) |\n",
    "| [t10k-images-idx3-ubyte.gz](http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz) | test set images (1648877 bytes) | \n",
    "| [t10k-labels-idx1-ubyte.gz](http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz) | test set labels (4542 bytes) | \n",
    "\n",
    "> The first 5000 examples of the test set are taken from the original NIST training set. The last 5000 are taken from the original NIST test set. The first 5000 are cleaner and easier than the last 5000.  \n",
    "\n",
    "### Training set label file (train-labels-idx1-ubyte)\n",
    "The labels values are 0 to 9.\n",
    "\n",
    "| offset        | type           | value               | description              |\n",
    "| :------------ | -------------- | ------------------- | ------------------------ |\n",
    "| 0000          | 32 bit integer | 0x00000801(2049)    | magic number (MSB first) |\n",
    "| 0004          | 32 bit integer | 60000               | number of items          |\n",
    "| 0008          | unsigned byte  | ??                  | label                    |\n",
    "| 0009          | unsigned byte  | ??                  | label                    |\n",
    "\n",
    "### Training set image file (train-images-idx3-ubyte):\n",
    "Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).\n",
    "\n",
    "| offset        | type           | value               | description              |\n",
    "| :------------ | -------------- | ------------------- | ------------------------ |\n",
    "| 0000          | 32 bit integer | 0x00000803(2051)    | magic number             |\n",
    "| 0004          | 32 bit integer | 60000               | number of images         |\n",
    "| 0008          | 32 bit integer | 28                  | number of rows           |\n",
    "| 0012          | 32 bit integer | 28                  | number of columns        |\n",
    "| 0016          | unsigned byte  | ??                  | pixel                    |\n",
    "| 0017          | unsigned byte  | ??                  | pixel                    |\n",
    "| xxxx          | unsigned byte  | ??                  | pixel                    |\n",
    "\n",
    "\n",
    "### Test set label file (t10k-labels-idx1-ubyte)\n",
    "The labels values are 0 to 9.\n",
    "\n",
    "| offset        | type           | value               | description              |\n",
    "| :------------ | -------------- | ------------------- | ------------------------ |\n",
    "| 0000          | 32 bit integer | 0x00000801(2049     | magic number (MSB first) |\n",
    "| 0004          | 32 bit integer | 10000               | number of items          |\n",
    "| 0008          | unsigned byte  | ??                  | label                    |\n",
    "| 0009          | unsigned byte  | ??                  | label                    |\n",
    "\n",
    "### Test set image file (t10k-images-idx3-ubyte):\n",
    "Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black). \n",
    "\n",
    "| offset        | type           | value               | description              |\n",
    "| :------------ | -------------- | ------------------- | ------------------------ |\n",
    "| 0000          | 32 bit integer | 0x00000803(2051)    | magic number             |\n",
    "| 0004          | 32 bit integer | 10000               | number of images         |\n",
    "| 0008          | 32 bit integer | 28                  | number of rows           |\n",
    "| 0012          | 32 bit integer | 28                  | number of columns        |\n",
    "| 0016          | unsigned byte  | ??                  | pixel                    |\n",
    "| 0017          | unsigned byte  | ??                  | pixel                    |\n",
    "| xxxx          | unsigned byte  | ??                  | pixel                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDX File Format\n",
    "The dataset is stored with the IDX file format, the full specification of the IDX file format can be found [here](http://www.fon.hum.uva.nl/praat/manual/IDX_file_format.html). \n",
    "\n",
    "> The IDX file format is a simple format for vectors and multidimensional matrices of various numerical types.\n",
    "\n",
    "### Example\n",
    "The training and testing data of the MNIST database of handwritten digits at [mnist](http://yann.lecun.com/exdb/mnist/) is stored in compressed IDX formatted files.\n",
    "\n",
    "Reading the uncompressed file train-images-idx3-ubyte available at [mnist](http://yann.lecun.com/exdb/mnist/) with 60000 images of 28×28 pixel data, will result in a new Matrix object with 60000 rows and 784 (=28×28) columns. \n",
    "\n",
    "Each cell will contain a number in the interval from 0 to 255.\n",
    "\n",
    "Reading the uncompressed file train-labels-idx1-ubyte with 60000 labels will result in a new Matrix object with 1 row and 60000 columns. Each cell will contain a number in the interval from 0 to 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset from file\n",
    "To read the dataset into memory we will use a widely supported python package called gzip to convert the file into bytes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Endian, Little Endian\n",
    "In the MNIST dataset specifcation as seen above, it states that the vectors are stored in big endian format. I will briefly explain what big endian means and why it is relavant to understand how it works when working with this dataset.\n",
    "\n",
    "#### Big Endian byte order\n",
    "The most significant byte (the \"big end\") of the data is placed at the byte with the lowest address. The rest of the data is placed in order in the next three bytes in memory.\n",
    "\n",
    "#### Little Endian byte order\n",
    "The least significant byte (the \"little end\") of the data is placed at the byte with the lowest address. The rest of the data is placed in order in the next three bytes in memory.\n",
    "\n",
    "For this dataset as we are using a machine with an Intel CPU we need to convert the dataset stored with big endian byter order to little endian. We need to do this otherwise we would not be able to manipulate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic number:  2051\n",
      "Number of images:  10000\n",
      "Number of rows:  28\n",
      "Number of columns:  28\n"
     ]
    }
   ],
   "source": [
    "import gzip \n",
    "\n",
    "# Open zip file with gzip\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    file_content = f.read()\n",
    "    \n",
    "# Read in first 4 bytes which we know is the magic number\n",
    "# Convert bytes to int with big endian byte order (most intel CPUs are big endian)\n",
    "magic = int.from_bytes(file_content[0:4], byteorder=\"big\")\n",
    "print(\"Magic number: \", magic)\n",
    "\n",
    "# Read in number of images\n",
    "images = int.from_bytes(file_content[4:8], byteorder=\"big\")\n",
    "print(\"Number of images: \", images)\n",
    "\n",
    "# Read in number of rows\n",
    "rows = int.from_bytes(file_content[8:12], byteorder=\"big\")\n",
    "print(\"Number of rows: \", rows)\n",
    "\n",
    "# Read in number of columns\n",
    "cols = int.from_bytes(file_content[12:16], byteorder=\"big\")\n",
    "print(\"Number of columns: \", cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display an Image\n",
    "We will use the matplotlib.pyplot package and the numpy package to display the first image in the dataset. The first image in the dataset is known to be a 7. We can see this by the plot below but can also validate this when we read in the labels from the labels file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10a479b00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# flip the bytes from black/white with tilda(~).\n",
    "image = ~np.array(list(file_content[16:800])).reshape(28,28).astype(np.uint8)\n",
    "\n",
    "# use the imshow method and map the image to gray scale\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic number is:  2049\n",
      "Number of labels:  10000\n",
      "First label:  7\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    labels = f.read()\n",
    "\n",
    "magic = int.from_bytes(labels[0:4], byteorder=\"big\")\n",
    "print(\"Magic number is: \", magic)\n",
    "\n",
    "# Read in number of labels\n",
    "num_labels = int.from_bytes(labels[4:8], byteorder=\"big\")\n",
    "print(\"Number of labels: \", num_labels)\n",
    "\n",
    "# Finally read in the first label and output to console\n",
    "first_label = int.from_bytes(labels[8:9], byteorder=\"big\")\n",
    "print(\"First label: \", first_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read entire dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# Read in entire training set with gzip\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "# Read in training labels\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_labels = f.read()\n",
    "\n",
    "# Read in training images\n",
    "# Convert images from black background and white foreground to white background and white foreground with tilda(~)\n",
    "# Reshape the array to 28 * 28 with numpy convert to unsigned 8 bit integer\n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000,28,28).astype(np.uint8)\n",
    "\n",
    "# Read in training labels\n",
    "# Convert to unsigned 8 bit integer\n",
    "train_labels = np.array(list(train_labels[8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For building neural networks.\n",
    "import keras as kr\n",
    "\n",
    "# For interacting with data sets.\n",
    "import pandas as pd\n",
    "\n",
    "# For encoding categorical variables.\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "# For splitting into training and test sets.\n",
    "import sklearn.model_selection as mod\n",
    "\n",
    "# Use keras Sequential model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# hidden layer with 1000 neurons and 784 input neurons\n",
    "# 784 pixels per image in MNIST\n",
    "model.add(kr.layers.Dense(units=1000, activation=\"relu\", input_dim=784))\n",
    "\n",
    "# 10 output layers for numbers 0-9 \n",
    "model.add(kr.layers.Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# Use categorical crossentropy as loss function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_img.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot encoding\n",
    "One Hot encoding is a pre proccessing process for data that is used for ML algorithms. It is mainly used for categorical data such as the MNIST dataset. One hot encoding is used because a majority of ML algorithms as they cannot use label data and they can only read data in a numeric format.\n",
    "\n",
    "In this example we will use One Hot encoding on the output data (the labels).\n",
    "\n",
    "The format of the output will look something like this if the label value is 5:\n",
    "\n",
    "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0] \n",
    "\n",
    "The digit 1 represents the label 5, all other positions in the array are set to 0. This is to indicate that they are not the label value. The label is the position where the 1 is located in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode the classes as above.\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "# One hot encode output labels\n",
    "outputs = encoder.transform(train_labels)\n",
    "# first digit is 5, represented by the 6th position in array with value 1\n",
    "print(train_labels[0], outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[1 0 0 0 0 0 0 0 0 0]]\n",
      "1 [[0 1 0 0 0 0 0 0 0 0]]\n",
      "2 [[0 0 1 0 0 0 0 0 0 0]]\n",
      "3 [[0 0 0 1 0 0 0 0 0 0]]\n",
      "4 [[0 0 0 0 1 0 0 0 0 0]]\n",
      "5 [[0 0 0 0 0 1 0 0 0 0]]\n",
      "6 [[0 0 0 0 0 0 1 0 0 0]]\n",
      "7 [[0 0 0 0 0 0 0 1 0 0]]\n",
      "8 [[0 0 0 0 0 0 0 0 1 0]]\n",
      "9 [[0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, encoder.transform([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 14.5262 - acc: 0.0988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30fb5c18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with inputs and outputs set number of epochs to 1 and batch size to 100\n",
    "model.fit(inputs, outputs, epochs=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_img = f.read()\n",
    "\n",
    "with gzip.open('data/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbl = f.read()\n",
    "    \n",
    "test_img = ~np.array(list(test_img[16:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_lbl =  np.array(list(test_lbl[ 8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoder.inverse_transform(model.predict(test_img)) == test_lbl).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_img[11:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "As we can see from above the model has an accuracy of 10%. This is the same accuarcy that a person would have if they were to try and guess a digit from the dataset. \n",
    "\n",
    "For the next part of this project I am going to build a covolutional nerual network to try and achieve a much higher accuarcy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "- [MNIST classification](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
